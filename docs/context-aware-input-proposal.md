# Context-Aware Input — 方案讨论记录

## 背景

用户按 FN 键触发输入框时，可能正处于以下状态之一：
- 选中了一段文字
- 选中了某个文件
- 鼠标正指向屏幕上的某个区域

当用户输入的内容指代了这些选中/指向的对象（例如"这个怎么改"、"帮我解释一下这段"），我们希望系统能自动将相关上下文附带发送给 OpenClaw，让它理解用户所指的具体内容。

## 讨论过的两个方案

### 方案 A：实时判断（已放弃）

在用户打字过程中，由一个 Agent 实时分析输入内容，判断是否与选中内容/指向区域有关联，有关联则自动附带。

放弃原因：
- 用户还没打完字，意图不完整，判断准确率低
- 需要额外的 AI 调用，增加延迟和复杂度
- 本质上是用一个 AI 来判断"要不要把上下文发给另一个 AI"，这个判断完全可以让后者自己做

### 方案 B：后置判断（采用）

FN 触发时，无条件采集当前上下文，发送时一并带上。由 OpenClaw 背后的 LLM 自行判断这些上下文是否与用户的问题相关。

采用原因：
- 实现简单，采集逻辑在 FN 触发时一次性完成
- LLM 天然擅长忽略无关上下文，不需要额外规则
- 零额外延迟，不影响打字体验
- 基于完整的用户输入判断关联性，准确率更高

## 方案 B 的处理流程

```
用户按 FN 键
    │
    ├─ 采集上下文
    │   ├─ 检测是否有选中文字（Accessibility API）
    │   ├─ 检测是否有选中文件
    │   └─ 截取鼠标光标所在区域的小截图
    │
    ├─ 用户正常输入、编辑
    │
    └─ 用户发送
        │
        └─ 将用户消息 + 采集到的上下文一起发送给 OpenClaw
            ├─ 选中的文字 → 作为文本附带
            ├─ 选中的文件 → 作为文件路径或内容附带
            └─ 光标区域截图 → 作为图片附带
```

## AIPointer 端的职责

- 在 FN 触发时采集上下文（选中文字、文件、区域截图）
- 将上下文以清晰的标签包装进消息，例如：
  - `[Selected Text]: ...`
  - `[Cursor Area Screenshot]: (image)`
  - `[Selected File]: /path/to/file`
- 与用户的实际输入内容一起发送

## 需要 OpenClaw 端确认的问题

1. 当消息中附带了额外的上下文信息（文字、截图），LLM 是否能自然地判断这些上下文与用户问题的关联性？是否需要在 prompt 层面做特殊处理？
2. 目前图片通道已经支持（OpenAI 格式 / Anthropic Messages API），文本上下文直接嵌入消息即可。文件路径的处理方式是什么？是否需要 OpenClaw 端读取文件内容，还是由 AIPointer 端读取后作为文本发送？
3. 如果附带的上下文较大（例如长文本或高分辨率截图），是否有 payload 限制需要注意？
